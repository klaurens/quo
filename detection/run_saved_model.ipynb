{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65e44651-1e10-478f-ab0d-ee6e7cd2c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from utils import mask_utils, box_utils\n",
    "from utils.object_detection import visualization_utils\n",
    "from pycocotools import mask as mask_api\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "# %time model = tf.saved_model.load('saved_models')\n",
    "# model = tf.saved_model.load('tf2model')\n",
    "\n",
    "def infer(image_pattern):\n",
    "    label_map_dict = {}\n",
    "    with open('datasets/fashionpedia_label_map.csv', 'r') as csv_file:\n",
    "          reader = csv_file.readlines()\n",
    "          for row in reader:\n",
    "            k, v = row.split(':')\n",
    "            id_index = int(k)\n",
    "            name = v\n",
    "            label_map_dict[id_index] = {\n",
    "                'id': id_index,\n",
    "                'name': name,\n",
    "            }\n",
    "    res = []\n",
    "    image_files = glob.glob(image_pattern)\n",
    "    for i, image_file in tqdm(enumerate(image_files), total = len(image_files)):\n",
    "        print(f'processing image {i}, {image_file}', end = '\\t\\t\\t\\t\\r')\n",
    "        \n",
    "        image = Image.open(image_file)    \n",
    "        image_array = np.array(image)[:,:,:3] #remove alpha channel\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_array, axis=0), dtype=tf.uint8)\n",
    "        \n",
    "        # Perform inference\n",
    "        \n",
    "        serving_default_fn = model.signatures['serving_default']\n",
    "        output_results = serving_default_fn(input_tensor)\n",
    "            \n",
    "        image_with_detections_list = []\n",
    "        \n",
    "                  \n",
    "        num_detections = int(output_results['num_detections'][0])\n",
    "        np_boxes = output_results['detection_boxes'][0, :num_detections]\n",
    "        width, height = image.size\n",
    "        \n",
    "        np_image_info = output_results['image_info'][0]\n",
    "        np_boxes = np_boxes / np.tile(np_image_info[1:2, :], (1, 2))\n",
    "        ymin, xmin, ymax, xmax = np.split(np_boxes, 4, axis=-1)\n",
    "        ymin = ymin * height\n",
    "        ymax = ymax * height\n",
    "        xmin = xmin * width\n",
    "        xmax = xmax * width\n",
    "        \n",
    "        np_boxes = np.concatenate([ymin, xmin, ymax, xmax], axis=-1)\n",
    "        np_scores = output_results['detection_scores'][0, :num_detections]\n",
    "        np_classes = output_results['detection_classes'][0, :num_detections]\n",
    "        np_classes = np_classes.numpy().astype(np.int32)\n",
    "        np_attributes = output_results['detection_attributes'][\n",
    "          0, :num_detections, :]\n",
    "        \n",
    "        np_masks = None\n",
    "        if 'detection_masks' in output_results:\n",
    "            np_masks = output_results['detection_masks'][0, :num_detections]\n",
    "            np_masks = mask_utils.paste_instance_masks(\n",
    "                np_masks, box_utils.yxyx_to_xywh(np_boxes), height, width)\n",
    "            encoded_masks = [\n",
    "                  mask_api.encode(np.asfortranarray(np_mask))\n",
    "                  for np_mask in list(np_masks)]\n",
    "        \n",
    "        np_image = (image_array.reshape(height, width, 3).astype(np.uint8))\n",
    "        image_with_detections = (\n",
    "                  visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                      np_image,\n",
    "                      np_boxes,\n",
    "                      np_classes,\n",
    "                      np_scores,\n",
    "                      label_map_dict,\n",
    "                      instance_masks=np_masks,\n",
    "                      use_normalized_coordinates=False,\n",
    "                      max_boxes_to_draw=15,\n",
    "                      min_score_thresh=0.3))\n",
    "        image_with_detections_list.append(image_with_detections)\n",
    "\n",
    "        res.append({\n",
    "                'image_file': image_file,\n",
    "                'boxes': np_boxes,\n",
    "                'classes': np_classes,\n",
    "                'scores': np_scores.numpy(),\n",
    "                'attributes': np_attributes,\n",
    "                'masks': encoded_masks,\n",
    "            })\n",
    "        # plt.imshow(image_with_detections_list[0])\n",
    "        # plt.show()\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6b508a-e956-46d9-82c3-615fd7ecf352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f42ad6cdacc4c25a943c1651f57e95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1527 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 23min 4sproject-cluster-index/loveflair_tops/top\\zozie-top-in-broken-white\\LF-622.jpg\t\t\t\t3-7aa2-4e3a-b50f-eedeeeaab9f5.jpg\t\t\t\t\t\t\t\t\tg\t\t\t\tba-0204-4bc5-8e83-b0f8bedec903.jpg\t\t\t\t\n",
      "Wall time: 14min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "output = infer('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18bbdcc5-108b-4eef-baf4-9739dfb45018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save('loveflair_output', (output), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "078161fd-dbf1-4806-9cc3-938e192fbe41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/fashionpedia_label_map.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     mapping \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43moutput\u001b[49m)):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# item_idx=25\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(output[item_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m'\u001b[39m])):    \n\u001b[0;32m     12\u001b[0m         \u001b[38;5;66;03m# if output[item_idx]['classes'][i] in (1,2,3):\u001b[39;00m\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;66;03m# 1:shirt, blouse\u001b[39;00m\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;66;03m# 2:top, t-shirt, sweatshirt\u001b[39;00m\n\u001b[0;32m     15\u001b[0m             \u001b[38;5;66;03m# 3:sweater\u001b[39;00m\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;28mprint\u001b[39m(mapping[output[item_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], output[item_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m][i],  output[item_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m][i])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "import pycocotools.mask as mask_util\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "with open('datasets/fashionpedia_label_map.csv', 'r') as f:\n",
    "    mapping = f.readlines()\n",
    "\n",
    "for item_idx in range(len(output)):\n",
    "    # item_idx=25\n",
    "    for i in range(len(output[item_idx]['masks'])):    \n",
    "        # if output[item_idx]['classes'][i] in (1,2,3):\n",
    "            # 1:shirt, blouse\n",
    "            # 2:top, t-shirt, sweatshirt\n",
    "            # 3:sweater\n",
    "    \n",
    "            print(mapping[output[item_idx]['classes'][i] - 1], output[item_idx]['boxes'][i],  output[item_idx]['scores'][i])\n",
    "            fig, ax = plt.subplots(1, 5, figsize=(10,10))\n",
    "            img = cv2.imread(output[item_idx]['image_file'])\n",
    "            image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            ax[0].imshow(image)\n",
    "            ax[0].set_title('Original')\n",
    "    \n",
    "             # Encoded mask string\n",
    "            encoded_mask_string = output[item_idx]['masks'][i]\n",
    "            \n",
    "            # Decode the mask\n",
    "            decoded_mask = mask_util.decode(encoded_mask_string)\n",
    "            \n",
    "            # Visualize the mask        \n",
    "            ax[1].imshow(decoded_mask)        \n",
    "            ax[1].set_title('Mask')\n",
    "    \n",
    "            masked_img = np.einsum('hwc,hw->hwc', image, decoded_mask)\n",
    "            masked_img[masked_img == 0] = 255\n",
    "            ax[2].imshow(masked_img)\n",
    "            ax[2].set_title('Masked')\n",
    "    \n",
    "            #Crop to bounding boxes        \n",
    "            y1, x1, y2, x2 = np.ceil(output[item_idx]['boxes'][i]).astype(int)\n",
    "            crop_img = image[y1:y2, x1:x2]\n",
    "            ax[3].imshow(crop_img)\n",
    "            ax[3].set_title('Cropped')\n",
    "    \n",
    "            crop_masked_img = masked_img[y1:y2, x1:x2]\n",
    "            crop_masked_img[crop_masked_img == 0] = 255\n",
    "            ax[4].imshow(crop_masked_img)\n",
    "            ax[4].set_title('Masked + Cropped')\n",
    "           \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
